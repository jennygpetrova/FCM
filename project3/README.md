This report presents several iterative methods to approximate solutions to a linear system of equations. Routines are developed to observe the behaviors of Richardson's First Order Stationary Stationary (RF) method, the Steepest Descent (SD) method, and the Conjugate Gradient (CG) method on the eigen-coordinate system defined by the eigenvectors of $A$, when $A \in \mathbb{R}^{n \times n}$ is a symmetric positive definite (SPD) matrix. We investigate the implementation of the Jacobi, (Forward) Gauss-Seidel (GS), and Symmetric Gauss-Seidel (SGS) stationary methods on several matrices $A \in \mathbb{R}^{n \times n}$ with varying properties (SPD, diagonally dominant, tri-diagonal). Lastly, these methods are scaled to large, sparse symmetric matrices stored efficiently in the compressed row storage (CSR) matrix representation. We consider the theoretical results of each algorithm, and test the algorithms on problems for which we know the solution (i.e. problems where convergence to the true solution is guaranteed). We will provide a comprehensive comparison of the iterative methods examined and discuss proper selection of iterative methods based on the matrix properties, problem size, and desired solution accuracy. We will also discuss the performance of each method and identify the most efficient method for various matrix types.
